name: Deploy SAM

on:
  push:
    branches: [ "main" ]

env:
  AWS_REGION: "us-east-1"
  AWS_ACCOUNT_ID: "117819748843"

permissions:
  contents: read

jobs:
  deploy_python:
    name: Deploy Python Package
    runs-on: ubuntu-latest
    environment: production
    container:
      image: public.ecr.aws/sam/build-python3.9:latest #amazonlinux:2 #continuumio/miniconda3:latest #public.ecr.aws/sam/build-python3.7:latest

    steps:

      - name: install tar
        run: yum install -y tar git 

      - name: Checkout
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Build python package
        run: |
          pip install awscli
          conda install conda-pack
          conda create --name pyspark_env --file src/app/requirements.txt python=3.9 ./
          conda activate
          conda pack -n pyspark_env -o pyspark_env.tar.gz
          aws s3 cp pyspark_env.tar.gz s3://deployment-zone-${AWS_ACCOUNT_ID}/weather_etl/

      # - name: Build
      #   run: |
      #     # install Python 3.9.9 and activate the venv
      #     yum install -y python3

      #     # create python venv with Python 3.9.9
      #     python -m venv pyspark_venv --copies 
      #     source pyspark_venv/bin/activate

      #     # Install application
      #     pip install ./


      #     # copy system python3 libraries to venv
      #     # cp -r /usr/local/lib/python/* ./pyspark_venv/lib/python3.9/

      #     # package venv to archive. 
      #     # **Note** that you have to supply --python-prefix option 
      #     # to make sure python starts with the path where your 
      #     # copied libraries are present.
      #     # Copying the python binary to the "environment" directory.
      #     pip install venv-pack
      #     venv-pack -f -o pyspark_env.tar.gz --python-prefix /home/hadoop/environment

      #     pip3 install awscli
      #     aws s3 cp pyspark_env.tar.gz s3://deployment-zone-${AWS_ACCOUNT_ID}/weather_etl/



      # - name: Build python package
      #   run: |
      #     pip install venv-pack
      #     python -m venv pyspark_venv 
      #     source pyspark_venv/bin/activate
      #     pip install ./
      #     venv-pack -f -o pyspark_env.tar.gz --python-prefix /home/hadoop/environment
      #     aws s3 cp pyspark_env.tar.gz s3://deployment-zone-${AWS_ACCOUNT_ID}/weather_etl/

      - name: Copy Python files to S3 deployment zone
        run: |
          aws s3 cp spark_entrypoint.py s3://deployment-zone-${AWS_ACCOUNT_ID}/weather_etl/

      


    
  deploy_sam:
    name: Deploy SAM template
    runs-on: ubuntu-latest
    environment: production
    container:
      image: public.ecr.aws/sam/build-python3.9:latest

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v1
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}


    - name: SAM build
      run: sam build
      
    - name: SAM deploy
      run: sam deploy


    # - name: SAM build
    #   run: cd lake_freeze_api/ && sam build
      
    # - name: SAM deploy
    #   run: cd lake_freeze_api/ && sam deploy